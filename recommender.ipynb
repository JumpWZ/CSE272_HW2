{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "from surprise import BaselineOnly, SVD, SlopeOne, NMF, CoClustering\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from metrics import precision_recall_at_k, get_conversion_rate, get_ndcg, f_measure\n",
    "from utils import output_ranking\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE_PATH = './Automotive.json'\n",
    "TRAIN_FILE_PATH = './train.csv'\n",
    "TEST_FILE_PATH = './test.csv'\n",
    "APPROACH = 'SVD'\n",
    "OUTPUT_RANKING_FILE = 'ranking'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(data_file_path):\n",
    "    data = []\n",
    "    print(f\"read {data_file_path}...\")\n",
    "    with open(data_file_path, \"r\") as data_file:\n",
    "        lines = data_file.readlines()\n",
    "        for line in tqdm(lines, desc=f\"read {data_file_path}\", total=len(lines)):\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def split_data(data):\n",
    "    user_ids_examples = {}\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    for example in tqdm(data, desc=\"generate user ids examples...\", total=len(data)):\n",
    "        if example[\"reviewerID\"] not in user_ids_examples:\n",
    "            user_ids_examples[example[\"reviewerID\"]] = [example]\n",
    "        else:\n",
    "            user_ids_examples[example[\"reviewerID\"]].append(example)\n",
    "    for user_ids in tqdm(user_ids_examples, desc=\"generate data split...\", total=len(user_ids_examples)):\n",
    "        examples = user_ids_examples[user_ids]\n",
    "        random.shuffle(examples)\n",
    "        train_size = int(len(examples)*0.8)\n",
    "        train_data.extend(examples[0:train_size])\n",
    "        test_data.extend(examples[train_size:])\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def convert_json_to_user_item_rating_csv(data, csv_file):\n",
    "    with open(csv_file, \"w\") as out_csv:\n",
    "        for example in tqdm(data, total=len(data), desc=f\"generate {csv_file}\"):\n",
    "            user_id = example[\"reviewerID\"]\n",
    "            item_id = example[\"asin\"]\n",
    "            rating = example[\"overall\"]\n",
    "            time = example[\"unixReviewTime\"]\n",
    "            out_csv.write(user_id+\"\\t\"+item_id+\"\\t\"+str(rating)+\"\\t\"+str(time)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read ./Automotive.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read ./Automotive.json: 100%|██████████| 7990166/7990166 [00:52<00:00, 152591.66it/s]\n",
      "generate user ids examples...: 100%|██████████| 7990166/7990166 [00:17<00:00, 448638.79it/s]\n",
      "generate data split...: 100%|██████████| 3873247/3873247 [00:07<00:00, 512881.40it/s]\n",
      "generate ./train.csv: 100%|██████████| 3739971/3739971 [00:06<00:00, 596385.33it/s]\n",
      "generate ./test.csv: 100%|██████████| 4250195/4250195 [00:06<00:00, 691884.30it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = read_json(DATA_FILE_PATH)\n",
    "train_data, test_data = split_data(dataset)\n",
    "convert_json_to_user_item_rating_csv(train_data, TRAIN_FILE_PATH)\n",
    "convert_json_to_user_item_rating_csv(test_data, TEST_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsl_options = {'method': 'sgd', 'n_epochs': 20, 'reg_u': 100, 'reg_i': 50}\n",
    "options = {\"SVD\": SVD(verbose=True, n_factors=20, n_epochs=3),\n",
    "            \"SlopeOne\": SlopeOne(),\n",
    "            \"NMF\": NMF(),\n",
    "            \"CoClustering\": CoClustering()}\n",
    "reader = Reader(line_format='user item rating timestamp', sep='\\t')\n",
    "algo = options[APPROACH]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.load_from_file(TRAIN_FILE_PATH, reader=reader)\n",
    "test_data = Dataset.load_from_file(TEST_FILE_PATH, reader=reader)\n",
    "train_set = train_data.build_full_trainset()\n",
    "test_set = test_data.build_full_trainset().build_testset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training....\n",
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "testing...\n",
      "MAE:  0.9900\n",
      "RMSE: 1.3408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3408185385479632"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"training....\")\n",
    "algo.fit(train_set)\n",
    "print(\"testing...\")\n",
    "predictions = algo.test(test_set)\n",
    "accuracy.mae(predictions, verbose=True)\n",
    "accuracy.rmse(predictions, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.dataset.DatasetAutoFolds at 0x7fe3202b3f10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating recommend_output...\n",
      "Precision: 0.8355845851551472\n",
      "Recall: 0.8397980780227251\n",
      "F-measure: 0.8376860332405245\n",
      "conversion_rate: 1.0\n",
      "ndcg: 0.9884684684684684\n"
     ]
    }
   ],
   "source": [
    "### Extra Credit\n",
    "output_ranking(predictions, OUTPUT_RANKING_FILE + \"_\" + APPROACH + \".out\")\n",
    "precisions, recalls = precision_recall_at_k(predictions, k=10, threshold=2.5)\n",
    "print(\"Precision:\", sum(prec for prec in precisions.values()) / len(precisions))\n",
    "print(\"Recall:\", sum(rec for rec in recalls.values()) / len(recalls))\n",
    "print(\"F-measure:\", f_measure(precisions, recalls))\n",
    "print(\"conversion_rate:\", get_conversion_rate(predictions, k=10))\n",
    "print(\"ndcg:\", get_ndcg(predictions, k_highest_scores=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
